针对在V100等volta架构的gpu上可以使用混合精度的方法进行训练(Mixed precision)。这种方法简言之就是利用float16较float32运算速度快的优点进行训练。

使用精度低于FP32的系统可以减少内存使用，允许部署更大的网络。数据传输需要更少的时间，而且计算性能会提高，尤其是在NVIDIA gpu上，它的Tensor Core支持这种精度。DNNs的混合精度训练实现了两个主要目标:

-减少需要的内存，使训练更大的模型或训练更大的小批量

-通过低精度算法降低所需资源，缩短训练/推理时间。

    混合精度训练可以提高计算性能，并在保持训练精度的同时减少内存带宽。
    充分利用了Tensor Cores在FP16中进行计算操作。
    权重的主副本保存在FP32中，以避免在反向传播期间进行不精确的权重更新。
    为了确保梯度在FP16中得到安全地表示，进行了损耗缩放，并在FP32中计算了损耗，以避免FP16中出现的溢出问题。
    Tensor Core加速的最佳实践指南:使用8的倍数做为Linear层矩阵的大小, 和做为卷积通道的数量.。
   
   
   深度学习减少计算资源有:
   模型压缩:1.模型剪枝2.降低模型参数的数值精度
   
   混合精度的必要性:
   因为半精度的表示范围为2^-24~65504.在语音识别中大约有5%的模型梯度小于2^-24,所以在更新梯度时需要单精度的类型.
   
/home/zhngqn/Pictures/Screenshot from 2019-01-17 09-17-55.png
